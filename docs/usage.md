# COAI LinkedIn Content Pipeline - Usage Guide

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9+
- PostgreSQL
- Redis
- Ollama (for local LLM)
- Docker & Docker Compose (optional, for containerized deployment)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/prof-schacht/coai-linkedin-content-pipeline.git
   cd coai-linkedin-content-pipeline
   ```

2. **Set up Python environment with uv:**
   ```bash
   # Install uv if not already installed
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # Initialize project and create virtual environment
   uv init --python 3.11
   uv venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate

   # Install dependencies
   uv pip install -r requirements.txt
   ```

3. **Configure environment variables:**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

4. **Verify services are running:**
   ```bash
   # Check PostgreSQL
   pg_isready -h localhost -p 5432

   # Check Redis
   redis-cli ping

   # Check Ollama
   curl http://localhost:11434/api/tags
   ```

### Testing the Setup

Run the LiteLLM connection test to verify everything is working:

```bash
python scripts/test_litellm_connection.py
```

This will test:
- âœ… Basic connection to Ollama
- âœ… Model listing
- âœ… Simple completion
- âœ… Model fallback
- âœ… Response timing

### Running Tests

Run the full test suite with pytest:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov=config

# Run specific test file
pytest tests/test_litellm_config.py -v
```

## ğŸ“¦ Project Structure

```
coai-linkedin-content-pipeline/
â”œâ”€â”€ config/               # Configuration modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ litellm_config.py # LiteLLM routing configuration
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ collectors/       # Data collection modules
â”‚   â”œâ”€â”€ agents/          # CrewAI agents
â”‚   â”œâ”€â”€ generators/      # Content generation
â”‚   â”œâ”€â”€ analyzers/       # Analysis tools
â”‚   â””â”€â”€ utils/           # Utility functions
â”œâ”€â”€ scripts/             # Executable scripts
â”‚   â””â”€â”€ test_litellm_connection.py
â”œâ”€â”€ tests/               # Test suite
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_litellm_config.py
â”œâ”€â”€ docs/                # Documentation
â”‚   â””â”€â”€ usage.md         # This file
â””â”€â”€ tmp/                 # Development notes
    â”œâ”€â”€ scratchpad.md
    â””â”€â”€ test_advice.md
```

## ğŸ”§ Configuration

### LiteLLM Configuration

The system uses LiteLLM for flexible LLM routing. Key features:

1. **Primary Model**: Ollama with deepseek-r1:1.5b (local, free)
2. **Fallback Models**: Cloud providers (OpenAI, Anthropic, Google)
3. **Automatic Failover**: If one model fails, tries the next
4. **Cost Tracking**: Monitors usage and alerts on budget threshold

### Environment Variables

Key environment variables in `.env`:

```bash
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
DEFAULT_MODEL=ollama/deepseek-r1:1.5b

# Model Priority (comma-separated)
MODEL_PRIORITY=ollama/deepseek-r1:1.5b,ollama/qwen3:8b,gpt-3.5-turbo

# Optional Cloud Providers
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here
GOOGLE_API_KEY=your_key_here

# Cost Management
MONTHLY_BUDGET_USD=100
COST_ALERT_THRESHOLD=0.8

# Database
DATABASE_URL=postgresql://postgres:password@localhost:5432/coai_pipeline
REDIS_URL=redis://localhost:6379/0
```

## ğŸ³ Docker Deployment

For production deployment, use Docker Compose:

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

Services included:
- PostgreSQL database
- Redis cache
- Ollama LLM server
- Celery workers
- Flower monitoring (optional)
- pgAdmin (optional, development only)

## ğŸ” Monitoring

### Cost Tracking

The system automatically tracks LLM usage costs:
- Free for Ollama models
- Tracks token usage for cloud models
- Alerts when approaching budget threshold

### Logging

Logs are configured at INFO level by default. To enable debug logging:

```bash
export LITELLM_VERBOSE=True
export LOG_LEVEL=DEBUG
```

## ğŸ› ï¸ Development

### Adding New Features

1. Create feature branch
2. Implement with tests
3. Update documentation
4. Create pull request

### Code Style

- Use `black` for formatting
- Follow PEP 8 guidelines
- Write comprehensive docstrings
- Add type hints where applicable

### Testing Guidelines

- Write pytest tests for all new functions
- Aim for >80% code coverage
- Use mocks for external dependencies
- Include both unit and integration tests

## ğŸ“š Next Steps

After setup is complete, proceed to implement:
1. arXiv paper collector (Issue #2)
2. X.com web scraper (Issue #3)
3. CrewAI agents (Issue #4)
4. LinkedIn network analyzer (Issue #5)
5. Content pipeline (Issue #6)
6. Monitoring system (Issue #7)

## ğŸ¤ Support

For issues or questions:
- Check the [GitHub Issues](https://github.com/prof-schacht/coai-linkedin-content-pipeline/issues)
- Review test output in `tmp/test_advice.md`
- Check development notes in `tmp/scratchpad.md`