#!/usr/bin/env python3
"""
Comprehensive test script for the content pipeline and monitoring system.
Tests all components of Issues #6 and #7.
"""

import os
import sys
import asyncio
from pathlib import Path
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.base import init_db, get_db
from src.models.generated_post import GeneratedPost
from src.generators.content_scorer import ContentScorer, ContentOpportunity
from src.generators.post_creator import ContentPipeline
from src.utils.cost_tracker import CostTracker
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


async def test_content_scoring():
    """Test the content scoring system."""
    print("\nğŸ¯ Testing Content Scoring System...")
    
    try:
        scorer = ContentScorer()
        
        # Test scoring recent content
        opportunities = scorer.score_recent_content(days=30)
        print(f"   âœ… Found {len(opportunities)} content opportunities")
        
        # Test getting top opportunities
        top_opportunities = scorer.get_top_opportunities(count=3, days=30)
        print(f"   âœ… Selected {len(top_opportunities)} top opportunities")
        
        if top_opportunities:
            best = top_opportunities[0]
            print(f"   ğŸ“Š Best opportunity: {best.title[:50]}... (Score: {best.total_score:.1f})")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Content scoring failed: {e}")
        return False


async def test_pipeline_orchestrator():
    """Test the main pipeline orchestrator."""
    print("\nğŸš€ Testing Pipeline Orchestrator...")
    
    try:
        pipeline = ContentPipeline()
        
        # Test pipeline statistics
        stats = pipeline.get_pipeline_stats(7)
        print(f"   âœ… Pipeline stats: {stats['total_posts_generated']} posts generated")
        print(f"   ğŸ“Š Approval rate: {stats['approval_rate']:.1%}")
        print(f"   â­ Average quality: {stats['average_quality_score']}")
        
        # Test emergency post generation
        print("   ğŸš¨ Testing emergency post generation...")
        emergency_post = await pipeline.generate_emergency_post(
            topic="AI Safety Breakthrough",
            urgency="high"
        )
        
        if emergency_post:
            print(f"   âœ… Emergency post created (ID: {emergency_post.id})")
        else:
            print("   âš ï¸ Emergency post not created (may be due to quality threshold)")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Pipeline orchestrator failed: {e}")
        return False


def test_cost_tracking():
    """Test the cost tracking system."""
    print("\nğŸ’° Testing Cost Tracking System...")
    
    try:
        tracker = CostTracker()
        
        # Test tracking a mock LLM call
        result = tracker.track_llm_call(
            model="ollama/deepseek-r1:1.5b",
            input_tokens=100,
            output_tokens=50,
            component="test_pipeline",
            request_type="completion",
            latency_ms=500,
            success=True
        )
        
        print(f"   âœ… Tracked LLM call: ${result['total_cost']:.6f}")
        
        # Test usage statistics
        stats = tracker.get_usage_stats(30)
        print(f"   ğŸ“Š Total requests (30d): {stats['total_requests']}")
        print(f"   ğŸ’µ Total cost (30d): ${stats['total_cost']:.4f}")
        print(f"   ğŸ¯ Success rate: {stats['success_rate']:.1%}")
        
        # Test monthly costs
        monthly = tracker.get_monthly_costs()
        print(f"   ğŸ“… Monthly cost: ${monthly['current_month_cost']:.4f}")
        print(f"   ğŸ“Š Budget usage: {monthly['budget_usage_percent']:.1%}")
        
        # Test recommendations
        recommendations = tracker.get_model_recommendations()
        print(f"   ğŸ’¡ Recommendations: {len(recommendations)}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Cost tracking failed: {e}")
        return False


def test_post_generation():
    """Test post generation and storage."""
    print("\nğŸ“ Testing Post Generation...")
    
    try:
        with get_db() as db:
            # Create a test post
            test_post = GeneratedPost(
                content="This is a test LinkedIn post about AI safety research. "
                       "It demonstrates the importance of alignment and interpretability. "
                       "What are your thoughts on the latest developments? #AISafety #MechanisticInterpretability",
                hashtags=["#AISafety", "#MechanisticInterpretability"],
                mentions=["@TestUser"],
                quality_score=8.5,
                status="draft",
                engagement_prediction=0.85
            )
            
            db.add(test_post)
            db.commit()
            db.refresh(test_post)
            
            print(f"   âœ… Created test post (ID: {test_post.id})")
            print(f"   ğŸ“Š Quality score: {test_post.quality_score}")
            print(f"   ğŸ“ Content length: {len(test_post.content)} chars")
            print(f"   ğŸ·ï¸ Hashtags: {len(test_post.hashtags)} tags")
            print(f"   ğŸ‘¥ Mentions: {len(test_post.mentions)} people")
            
            # Test post properties
            print(f"   âœ… Ready to post: {test_post.is_ready_to_post}")
            
            return True
            
    except Exception as e:
        print(f"   âŒ Post generation failed: {e}")
        return False


def test_database_connectivity():
    """Test database connectivity and models."""
    print("\nğŸ—„ï¸ Testing Database Connectivity...")
    
    try:
        # Test connection
        init_db()
        print("   âœ… Database connection successful")
        
        # Test table existence
        with get_db() as db:
            from sqlalchemy import inspect
            inspector = inspect(db.bind)
            tables = inspector.get_table_names()
            
            required_tables = [
                'papers', 'x_posts', 'linkedin_connections', 
                'generated_posts', 'post_analytics', 'cost_records'
            ]
            
            missing_tables = []
            for table in required_tables:
                if table in tables:
                    print(f"   âœ… Table '{table}' exists")
                else:
                    missing_tables.append(table)
                    print(f"   âŒ Table '{table}' missing")
            
            if missing_tables:
                print(f"   âš ï¸ Missing tables: {missing_tables}")
                return False
            
            # Test basic queries
            post_count = db.query(GeneratedPost).count()
            print(f"   ğŸ“Š Generated posts in database: {post_count}")
            
            return True
            
    except Exception as e:
        print(f"   âŒ Database connectivity failed: {e}")
        return False


def test_visual_extraction():
    """Test visual content extraction capabilities."""
    print("\nğŸ–¼ï¸ Testing Visual Content Extraction...")
    
    try:
        from src.generators.visual_extractor import VisualExtractor
        
        extractor = VisualExtractor()
        
        # Test quote card creation
        quote_card = extractor.create_quote_card(
            text="AI safety research is crucial for ensuring that artificial general intelligence systems remain aligned with human values.",
            author="COAI Research Team",
            source="Research Brief 2024",
            theme="professional"
        )
        
        if quote_card:
            print(f"   âœ… Quote card created: {quote_card['filename']}")
            print(f"   ğŸ“ Dimensions: {quote_card['width']}x{quote_card['height']}")
            print(f"   ğŸ’¾ File size: {quote_card['file_size']} bytes")
        else:
            print("   âš ï¸ Quote card creation skipped (PIL not available)")
        
        # Test visual cleanup
        cleaned = extractor.cleanup_old_visuals(days=30)
        print(f"   ğŸ§¹ Cleaned up {cleaned} old visual files")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Visual extraction failed: {e}")
        return False


async def run_comprehensive_tests():
    """Run all comprehensive tests."""
    print("ğŸ§ª COAI Content Pipeline - Comprehensive Test Suite")
    print("=" * 60)
    
    test_results = {}
    
    # Database tests
    test_results['database'] = test_database_connectivity()
    
    # Content scoring tests
    test_results['content_scoring'] = await test_content_scoring()
    
    # Post generation tests
    test_results['post_generation'] = test_post_generation()
    
    # Pipeline orchestrator tests
    test_results['pipeline'] = await test_pipeline_orchestrator()
    
    # Cost tracking tests
    test_results['cost_tracking'] = test_cost_tracking()
    
    # Visual extraction tests
    test_results['visual_extraction'] = test_visual_extraction()
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š TEST SUMMARY")
    print("-" * 30)
    
    passed = sum(test_results.values())
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {test_name.replace('_', ' ').title()}: {status}")
    
    print(f"\nğŸ¯ Overall Result: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! Pipeline is ready for production.")
        return True
    else:
        print("âš ï¸ Some tests failed. Please review the issues above.")
        return False


async def main():
    """Main execution function."""
    try:
        success = await run_comprehensive_tests()
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Tests interrupted by user")
        return 0
    except Exception as e:
        print(f"\nğŸ’¥ Test suite failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))